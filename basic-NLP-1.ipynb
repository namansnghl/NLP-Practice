{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP with nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Topics\n",
    "- [Tokenization](#1)\n",
    "- [Stop Word](#2)\n",
    "- [Frequency Distribution](#3)\n",
    "- [Lemmitization](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guilt is only useful when it causes us to change our behaviour and to make amends. \n",
      "Feelings of shame are not just personal. We can also pick up feelings of shame from our nation, \n",
      "family, or our even just being in the vicinity of an event.\n"
     ]
    }
   ],
   "source": [
    "input = '''Guilt is only useful when it causes us to change our behaviour and to make amends. \n",
    "Feelings of shame are not just personal. We can also pick up feelings of shame from our nation, \n",
    "family, or our even just being in the vicinity of an event.'''\n",
    "\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work and Sentence Tokenization<a id='1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer Output:\n",
      "['Guilt', 'is', 'only', 'useful', 'when', 'it', 'causes', 'us', 'to', 'change', 'our', 'behaviour', 'and', 'to', 'make', 'amends', '.', 'Feelings', 'of', 'shame', 'are', 'not', 'just', 'personal', '.', 'We', 'can', 'also', 'pick', 'up', 'feelings', 'of', 'shame', 'from', 'our', 'nation', ',', 'family', ',', 'or', 'our', 'even', 'just', 'being', 'in', 'the', 'vicinity', 'of', 'an', 'event', '.']\n",
      "\n",
      "Sentence Tokenizer Output:\n",
      "['Guilt is only useful when it causes us to change our behaviour and to make amends.', 'Feelings of shame are not just personal.', 'We can also pick up feelings of shame from our nation, \\nfamily, or our even just being in the vicinity of an event.']\n"
     ]
    }
   ],
   "source": [
    "#for reference. Download 'punkt' for tokenize operations\n",
    "from nltk import tokenize\n",
    "word = tokenize.word_tokenize(input)\n",
    "sentence = tokenize.sent_tokenize(input)\n",
    "\n",
    "print('Word Tokenizer Output:\\n{}\\n\\nSentence Tokenizer Output:\\n{}'.format(word,sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n"
     ]
    }
   ],
   "source": [
    "#fetching stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english')) \n",
    "print(stop_words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guilt', 'useful', 'causes', 'us', 'change', 'behaviour', 'make', 'amends', 'Feelings', 'shame', 'personal', 'We', 'also', 'pick', 'feelings', 'shame', 'nation', 'family', 'even', 'vicinity', 'event']\n"
     ]
    }
   ],
   "source": [
    "#removing punctuations and stop words\n",
    "filter_sentence = []\n",
    "for w in word:\n",
    "    if w in stop_words or len(w)==1:\n",
    "        pass\n",
    "    else:\n",
    "        filter_sentence.append(w)\n",
    "print(filter_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution<a id='3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import pandas as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = dict(FreqDist(filter_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Guilt</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>causes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>behaviour</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amends</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feelings</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shame</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pick</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelings</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicinity</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Count\n",
       "Guilt          1\n",
       "useful         1\n",
       "causes         1\n",
       "us             1\n",
       "change         1\n",
       "behaviour      1\n",
       "make           1\n",
       "amends         1\n",
       "Feelings       1\n",
       "shame          2\n",
       "personal       1\n",
       "We             1\n",
       "also           1\n",
       "pick           1\n",
       "feelings       1\n",
       "nation         1\n",
       "family         1\n",
       "even           1\n",
       "vicinity       1\n",
       "event          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.DataFrame(fdist.values(),index=fdist.keys(),columns=['Count'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmitization<a id='4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmat = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guilt  -  Guilt\n",
      "useful  -  useful\n",
      "causes  -  cause\n",
      "us  -  u\n",
      "change  -  change\n",
      "behaviour  -  behaviour\n",
      "make  -  make\n",
      "amends  -  amends\n",
      "Feelings  -  Feelings\n",
      "shame  -  shame\n",
      "personal  -  personal\n",
      "We  -  We\n",
      "also  -  also\n",
      "pick  -  pick\n",
      "feelings  -  feeling\n",
      "shame  -  shame\n",
      "nation  -  nation\n",
      "family  -  family\n",
      "even  -  even\n",
      "vicinity  -  vicinity\n",
      "event  -  event\n"
     ]
    }
   ],
   "source": [
    "for i in filter_sentence:\n",
    "    print(i,' - ',lemmat.lemmatize(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
